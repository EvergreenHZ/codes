{\rtf1\ansi\ansicpg1252\deff0\uc1
{\fonttbl
{\f0\fnil\fcharset0\fprq0\fttruetype Tinos;}
{\f1\fnil\fcharset0\fprq0\fttruetype Arimo;}
{\f2\fnil\fcharset0\fprq0\fttruetype Courier New;}}
{\colortbl
\red0\green0\blue0;
\red255\green255\blue255;
\red255\green255\blue255;}
{\stylesheet
{\s6\fi-431\li720\sbasedon28\snext28 Contents 1;}
{\s7\fi-431\li1440\sbasedon28\snext28 Contents 2;}
{\s1\fi-431\li720 Arrowhead List;}
{\s27\fi-431\li720\sbasedon28 Lower Roman List;}
{\s29\tx431\sbasedon20\snext28 Numbered Heading 1;}
{\s30\tx431\sbasedon21\snext28 Numbered Heading 2;}
{\s12\fi-431\li720 Diamond List;}
{\s9\fi-431\li2880\sbasedon28\snext28 Contents 4;}
{\s8\fi-431\li2160\sbasedon28\snext28 Contents 3;}
{\s31\tx431\sbasedon22\snext28 Numbered Heading 3;}
{\s32\fi-431\li720 Numbered List;}
{\s15\sbasedon28 Endnote Text;}
{\*\cs14\fs20\super Endnote Reference;}
{\s4\fi-431\li720 Bullet List;}
{\s5\tx1584\sbasedon29\snext28 Chapter Heading;}
{\s35\fi-431\li720 Square List;}
{\s11\fi-431\li720 Dashed List;}
{\s22\sb440\sa60\f1\fs24\b\sbasedon28\snext28 Heading 3;}
{\s37\fi-431\li720 Tick List;}
{\s24\fi-431\li720 Heart List;}
{\s40\fi-431\li720\sbasedon32 Upper Roman List;}
{\s39\fi-431\li720\sbasedon32 Upper Case List;}
{\s16\fi-288\li288\fs20\sbasedon28 Footnote;}
{\s19\fi-431\li720 Hand List;}
{\s18\fs20\sbasedon28 Footnote Text;}
{\s20\sb440\sa60\f1\fs34\b\sbasedon28\snext28 Heading 1;}
{\s21\sb440\sa60\f1\fs28\b\sbasedon28\snext28 Heading 2;}
{\s10\qc\sb240\sa120\f1\fs32\b\sbasedon28\snext28 Contents Header;}
{\s23\sb440\sa60\f1\fs24\b\sbasedon28\snext28 Heading 4;}
{\s28\f0\fs24 Normal;}
{\s26\fi-431\li720\sbasedon32 Lower Case List;}
{\s2\li1440\ri1440\sa120\sbasedon28 Block Text;}
{\s33\f2\sbasedon28 Plain Text;}
{\s34\tx1584\sbasedon29\snext28 Section Heading;}
{\s25\fi-431\li720 Implies List;}
{\s3\fi-431\li720 Box List;}
{\s36\fi-431\li720 Star List;}
{\*\cs17\fs20\super Footnote Reference;}
{\s38\fi-431\li720 Triangle List;}
{\s13\fi-288\li288\sbasedon28 Endnote;}}
\kerning0\cf0\ftnbj\fet2\ftnstart1\ftnnar\aftnnar\ftnstart1\aftnstart1\aenddoc\revprop3{\*\rdf}{\info\uc1}\deftab720\viewkind1\paperw11905\paperh16837\margl1440\margr1440\widowctrl
\sectd\sbknone\colsx360\headery0\footery0\pgncont\ltrsect
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/264558859}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}A Fast and Accurate Unconstrained Face Detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Article in IEEE Transactions on Pattern Analysis and Machine Intelligence \'b7 August 2014}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}DOI: 10.1109/TPAMI.2015.2448075 \'b7 Source: arXiv}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}CITATIONS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}READS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}126}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2,020}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3 authors, including:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Shengcai Liao}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Stan Z Li}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Chinese Academy of Sciences}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Chinese Academy of Sciences}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}68 PUBLICATIONS 4,432 CITATIONS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}790 PUBLICATIONS 25,075 CITATIONS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}SEE PROFILE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Some of the authors of this publication are also working on these related projects:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face recognition View project}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Face Detection View project}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}All content following this page was uploaded by Stan Z Li on 25 August 2014.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The user has requested enhancement of the downloaded file.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}SEE PROFILE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}A Fast and Accurate Unconstrained Face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}arXiv:1408.1656v1 [cs.CV] 6 Aug 2014}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Shengcai Liao, Member, IEEE, Anil K. Jain, Fellow, IEEE, and Stan Z. Li, Fellow, IEEE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Abstract\uc1\u8212\'97We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}scale invariant, bounded, and is able to reconstruct the original image. Second, we learn the optimal subset of NPD features and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}their combinations via regression trees, so that complex face manifolds can be partitioned by the learned rules. This way, only}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a single cascade classi\uc0\u-1279 er is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}be ef\uc0\u-1279 ciently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}very fast (about 178 FPS for 640x480 resolution videos and 30 FPS for 1920x1080 resolution videos on a desktop PC, about}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}6 times faster than OpenCV). Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed method outperforms the state-of-the-art methods in detecting unconstrained faces with arbitrary pose variations and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusions in cluttered scenes.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Index Terms\uc1\u8212\'97Unconstrained face detection, normalized pixel difference, regression tree, AdaBoost, cascade classi\uc0\u-1279 er}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}?}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}I NTRODUCTION}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The objective of face detection is to \uc0\u-1279 nd and locate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces in an image. It is the \uc0\u-1279 rst step in automatic face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}recognition applications. Face detection has been well}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}studied for frontal and near frontal faces. The Viola}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and Jones\uc1\u8217\'92 face detector [1] is the most well known}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection algorithm, which is based on Haar-like}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features and cascade AdaBoost [2] classi\uc0\u-1279 er. However,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in unconstrained scenes such as faces in a crowd,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}state-of-the-art face detectors fail to perform well}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}due to large pose variations, illumination variations,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusions, expression variations, out-of-focus blur,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and low image resolution. For example, the ViolaJones face detector fails to detect most of the face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images in the Face Detection Data set and Benchmark}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(FDDB) database [3] (examples shown in Fig. 1) due}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to the dif\uc0\u-1279 culties mentioned above. In this paper, we}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}refer to face detection with arbitrary facial variations}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}as the unconstrained face detection problem. We are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}interested in face detection in unconstrained scenarios}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}such as video surveillance or images captured by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}hand-held devices.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Numerous face detection methods have been developed following Viola and Jones\uc1\u8217\'92 work [1], mainly}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 Shengcai Liao and Stan Z. Li are with the National Laboratory of Pattern Recognition and the Center for Biometrics and Security Research,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Institute of Automation, Chinese Academy of Sciences, Beijing 100190,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}China. E-mail: \{scliao,szli\}@nlpr.ia.ac.cn}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 Anil K. Jain is with the Dept. of Computer Science and Engineering,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Michigan State University, East Lansing, MI 48824 USA. He is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}also af\uc0\u-1279 liated with the Dept. of Brain & Cognitive Engineering,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Korea University, Anamdong, Seongbukgu, Seoul 136-713, Republic}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of Korea. E-mail: jain@cse.msu.edu}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 1. Face images annotated (red ellipses) in the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FDDB database [3].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}focusing on extracting different types of features and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}developing different cascade structures. A variety of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}complex features [4], [5], [6], [7], [8], [9], [10], [11],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[12], [13] have been proposed to replace the Haarlike features used in [1]. While these methods can}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}improve the face detection performance to some extent, they generate a very large number (hundreds of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}thousands) of features and the resulting systems take}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}too much time to train. Another development in face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection has been to learn different cascade structures for multiview face detection, such as parallel}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade [14], pyramid architecture [15], and WidthFirst-Search (WFS) tree [16]. All these methods need}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to learn one cascade classi\uc0\u-1279 er for each speci\uc0\u-1279 c facial}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}view (or view range). In unconstrained scenarios,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}however, it is not easy to de\uc0\u-1279 ne all possible views}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of a face, and the computational cost increases with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}an increasing number of classi\uc0\u-1279 ers in complex cascade}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}structure. Moreover, these approaches require manual}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}labeling of face pose in each training image.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}While some of the available methods [14], [15],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[16] can handle multiview faces, they are not able}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to simultaneously consider other challenges such as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusion. In fact, since these methods require partitioning multiview data into known poses, occlusion}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is not easy to handle in this way. On the other hand,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}while several studies addressed face detection under}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusion [17], [18], [19], [20], [21], they constrained}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}themselves to detect only frontal faces under occlusion. As discussed in [22], a robust face detection algorithm should be effective under arbitrary variations}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in pose and occlusion, which remains an unresolved}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}challenging problem.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In this paper, we are interested in developing effective features and robust classi\uc0\u-1279 ers for unconstrained}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection with arbitrary facial variation. First, we}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}propose a simple pixel-level feature, called the Normalized Pixel Difference (NPD). An NPD is computed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}as the ratio of the difference between any two pixel}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}intensity values to the sum of their values, in the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}same form as the Weber Fraction in experimental psychology [23]. The NPD feature has several desirable}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}properties, such as scale invariance, boundedness, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ability to reconstruct the original image. we further}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}show that NPD features can be obtained from a look}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}up table, and the resulting face detection template can}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}be easily scaled for multiscale face detection.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Secondly, we develop a method to construct a single}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade AdaBoost classi\uc0\u-1279 er that can effectively deal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with complex face manifolds and handle arbitrary}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pose and occlusion conditions. While the individual}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD feature may have \uc1\u8220\'93weak\uc1\u8221\'94 discriminative ability,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}our work indicates that a subset of NPD features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can be optimally learned and combined to construct}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}more discriminative features in a regression tree. In}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}this way, different types of faces can be automatically}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}divided into different leaves of a regression tree, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the complex face manifold in high dimensional space}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can be partitioned in the learning process. This is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a \uc1\u8220\'93divide and conquer\uc1\u8221\'94 strategy to tackle unconstrained face detection in a single classi\uc0\u-1279 er, without}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pre-labeling of views in the training set of face images.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The resulting face detector is robust to variations in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pose, occlusion, and illumination, as well as to blur}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and low image resolution.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The novelty of this work is summarized as follows:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 A new type of feature, called NPD is proposed,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is ef\uc0\u-1279 cient to compute and has several}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}desirable properties, including scale invariance,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}boundedness, and enabling reconstruction of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}original image.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 A subset of NPD features is automatically learned}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and combined in regression trees to boost their}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discriminability. In this way, only a single cascade AdaBoost classi\uc0\u-1279 er is needed to handle unconstrained faces with occlusions and arbitrary}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}viewpoints, without pose labeling or clustering}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in the training stage.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The advantages of the proposed approach include:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 The NPD feature evaluation is extremely fast,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}requiring a single memory access using a look}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}up table.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 Multiscale face detection can be easily achieved}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}by applying pre-scaled detection templates.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 The unconstrained face detector does not depend}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on pose speci\uc0\u-1279 c cascade structure design; pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}labeling or clustering in the training stage is also}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not required.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 The face detector is able to handle illumination variations, pose variations, occlusions, outof-focus blur, and low resolution face images in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}unconstrained scenarios.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The remainder of this paper is organized as follows.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In Section 2 we review the related work. In Section 3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}we introduce the NPD feature space. The proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD based face detection method is presented in Section 4. Experimental results are provided in Section 5.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Finally, we summarize the contributions in Section 6.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}R ELATED}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}WORK}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}As indicated in a survey of face detection methods [24], the most popular face detection methods are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}appearance based, which use local feature representation and classi\uc0\u-1279 er learning. Viola and Jones\uc1\u8217\'92 face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detector [1] was the \uc0\u-1279 rst one to apply rectangular}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar-like features in a cascaded AdaBoost classi\uc0\u-1279 er}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for real-time face detection. Many approaches have}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}been proposed around the Viola-Jones detector to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}advance the state of the art in face detection. Lienhart}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and Maydt [4] proposed an extended set of Haar-like}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features, where 45\uc0\u9702  rotated rectangular features were}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}introduced. Li et al. [5] proposed another extension}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of Haar-like features, where the rectangles can be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}spatially set apart with a \uc0\u-1278 exible distance. A similar}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature, called the diagonal \uc0\u-1279 lter was also proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}by Jones and Viola [6]. Various other local texture}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features have been introduced for face detection, such}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}as the modi\uc0\u-1279 ed census transform [7], local binary}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pattern (LBP) [8], MB-LBP [11], LBP histogram [10],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and the locally assembled binary feature [12]. These}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features have been shown to be robust to illumination}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}variations. Mita et al. [9] proposed the joint Haarlike features to capture the co-occurrence of effective}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar-like features. Huang et al. [16] proposed a sparse}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature set in a granular space, where granules were}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}represented by rectangles, and each individual sparse}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature was learned as a combination of granules. A}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}problem with the approaches in [9] and [16] is that the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}joint feature space is very large, making the optimal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}combination a dif\uc0\u-1279 cult task.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}While more sophisticated features may provide better discrimination power than Haar-like features for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the face detection task, they generally increase the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}computational cost. In contrast, ordinal relationships}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}among image regions are simple yet effective image}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features [25], [26], [27], [28], [29], [30]. Sinha [25] studied several robust ordinal relationships in face images}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and developed a face detection method accordingly.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Liao et al. [28] further showed that ordinal features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can be effectively learned by AdaBoost classi\uc0\u-1279 er for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face recognition. Sadr et al. [26] showed that pixelwise}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ordinal features (ordinal relationship between any two}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pixels) can faithfully encode image structures. Baluja}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}et al. [27] showed that simple pixelwise ordinal features are good enough for discriminating between \uc0\u-1279 ve}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}facial orientations, a relatively simpler task than face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection. Wang et al. [30] applied the random forest}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er together with pixelwise ordinal features for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}facial landmark localization. Abramson and Steux [29]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed a pixel control point based feature for face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection, where each feature is associated with two}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}sets of pixel locations (control points). However, it is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not easy to learn control point based features because}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the huge number of control point combinations.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Besides different feature representations, some researchers have also tried different AdaBoost algorithms and weak classi\uc0\u-1279 ers. For weak classi\uc0\u-1279 ers utilized in boosting, Lienhart et al. [31] and Brubaker et}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}al. [32] have shown that classi\uc0\u-1279 cation and regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}trees (CART) [33] work better than simple decision}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}stumps. In this paper, we show that the optimal ordinal features and their combinations can be learned by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}integrating the proposed NPD features in a regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tree. In this way, unconstrained face variations can be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}automatically partitioned into different leaves of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}learned regression tree.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Given that the original Viola-Jones face detector has}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}limitations for multiview face detection [24], various}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade structures have been proposed to tackle multiview face detection [6], [14], [15], [16]. Jones and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Viola [6] extended their face detector by training one}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector for each speci\uc0\u-1279 c pose. To avoid evaluating all face detectors on each scanning subwindow,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}they developed a pose estimation step (similar to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Rowley et al. [34]) before face detection, and then only}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the face detector trained on that estimated pose was}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}applied. In this two-stage detection structure, if the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pose estimation is not reliable, the face is not likely}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to be detected in the second stage. Wu et al. [14]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed a parallel cascade structure for multiview}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection, where all face detectors tuned to different views have to be evaluated for each scanning}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}window; they did use the \uc0\u-1279 rst few cascade layers of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}all face detectors to estimate the pose for speedup.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Li and Zhang [15] proposed a coarse-to-\uc0\u-1279 ne pyramid}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}architecture for multiview face detection, where the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}entire range of face poses was divided into increasingly smaller subranges, resulting in a more ef\uc0\u-1279 cient}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection structure. Huang et al. proposed a WFS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tree based multiview face detection approach, which}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}also works in a coarse-to-\uc0\u-1279 ne manner. They proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the Vector Boost algorithm for multiclass learning,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is well suited for multiview pose estimation.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}However, all these methods need to learn a cascade}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er for each speci\uc0\u-1279 c view (or view range) of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a face, requiring an input face image to go through}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}different branches of the detection structure. Hence,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}their computational cost generally increases with the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}number of classi\uc0\u-1279 ers in complex cascade structures.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Moreover, these approaches require manual labeling}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the face pose in each training image.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Instead of designing a detection structure, Lin and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Liu [19] proposed to learn the multiview face detector as a single cascade classi\uc0\u-1279 er. They derived a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}multiclass boosting algorithm, called MBHBoost by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}sharing features among different classes. This is a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}simpler approach to multiview face detection than}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}designing complex cascade structures. Nevertheless, it}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}still requires manual labeling of poses. In uncontrolled}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}environments, however, it is not easy to de\uc0\u-1279 ne speci\uc0\u-1279 c}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}views of a face by discretizing the pose space, because}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a face could be in arbitrary pose simultaneously in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}yaw (out-of-plane), roll (in-plane), and pitch (up-anddown) angles. To avoid manual labeling, Seemann et}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}al. [35] suggested learning viewpoint clusters automatically for object detection. However, for human}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces, Kim and Cipolla [36] showed that clustering by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}traditional techniques like K-Means does not result}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in categorized poses. They hence proposed a multiclassi\uc0\u-1279 er boosting (MCBoost) for human perceptual}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}clustering of object images, which showed promise}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for clustering face poses. However, the clusters are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not always related to pose variations; in addition to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}different pose clusters, they also obtained clusters}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with various illumination variations.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Face detection in presence of occlusion is also an}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}important issue in unconstrained face detection, but}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}it has received less attention compared to multiview}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection. This is probably because, compared}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to pose variations, it is more dif\uc0\u-1279 cult to categorize}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}arbitrary occlusions into prede\uc0\u-1279 ned classes. Hotta [17]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed a local kernel based SVM method for face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection, which was better than global kernel based}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}SVM in detecting occluded frontal faces. Lin et al. [18]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}considered 8 kinds of manually de\uc0\u-1279 ned facial occlusions by training 8 additional cascade classi\uc0\u-1279 ers}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}besides the standard face detector. Lin and Liu [19]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}further proposed the MBHBoost algorithm to handle}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces with one of 12 in-plane rotations or one of 8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}types of occlusions, with each kind of rotation and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusion treated as a different class. Chen et al. [20]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed a modi\uc0\u-1279 ed Viola-Jones face detector, where}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the trained detector was divided into sub-classi\uc0\u-1279 ers}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}related to several prede\uc0\u-1279 ned local patches, and the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}outputs of sub-classi\uc0\u-1279 ers were fused. Goldmann et}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}al. [21] proposed a component-based approach for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection, where the two eyes, nose, and mouth}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}were detected separately, and further connected in a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}topology graph. However, none of the above meth-}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ods considered face detection with both occlusions}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and pose variations simultaneously in unconstrained}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}scenarios. As discussed in [22], a robust face detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}should be effective under arbitrary variations in pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and occlusion, which has not yet been solved.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Recently, unconstrained face detection has gained}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}attention. Jain and Learned-Miller [3] developed the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FDDB database and benchmark for the development of unconstrained face detection algorithms. This}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database contains images collected from the Internet,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and presents challenging scenarios for face detection.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Subburaman and Marcel [37] proposed a fast bounding box estimation technique for face detection, where}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the bounding box is predicted by small patch based}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}local search. Jain and Learned-Miller [38] proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}an online domain adaption approach to improve}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the performance of the Viola-Jones face detector on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the FDDB database. Li et al. [13] proposed the use}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of SURF feature [39] in an AdaBoost cascade, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}area under the curve (AUC) criterion to speed up}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the face detector training. Zhu and Ramanan [40]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed to jointly detect face, estimate pose, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}localize face landmarks in the wild. Shen et al. [41]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed an exemplar-based face detection approach,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which retrieves images from a large annotated face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}dataset; facial landmark locations are inferred from}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the annotations. Li et al. [42] proposed a probabilistic}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}elastic part (PEP) model to adapt any pre-trained}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector to a speci\uc0\u-1279 c image collection like FDDB.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This method extracts the PEP representation for each}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}candidate face detected by a general face detector, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}trains a classi\uc0\u-1279 er with the top positive and negative}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}samples. Despite the availability of these methods for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}unconstrained face detection, the detection accuracy}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is still not satisfactory, especially when the detector is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}required to have low false alarms.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}N ORMALIZED P IXEL D IFFERENCE F EA TURE S PACE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The Normalized Pixel Difference (NPD) feature between two pixels in an image is de\uc0\u-1279 ned as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0},}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(1)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}where x, y \uc0\u8805  0 are intensity values of the two pixels1 ,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and f (0, 0) is de\uc0\u-1279 ned as 0 when x = y = 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The NPD feature measures the relative difference}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}between two pixel values. The sign of f (x, y) indicates}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the ordinal relationship between the two pixels x and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}y , and the magnitude of f (x, y) measures the relative}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}difference (as a percentage of the joint intensity x + y)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}between x and y. Note that the de\uc0\u-1279 nition f (0, 0) ? 0 is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}reasonable because, in this case, there is no difference}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1. For ease of representation, sometimes we also denote x and y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}as pixels instead of pixel values. We use subscripts to differentiate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}between pixel and pixel values only when pixel locations are under}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discussion.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}between the two pixels x and y. Compared to the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}absolute difference |x \uc0\u8722  y|, NPD is invariant to scale}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}change of the pixel intensities.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Weber, a pioneer in experimental psychology, stated}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}that the just-noticeable difference in the magnitude}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}change of a stimulus is proportional to the magnitude}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the stimulus, rather than its absolute value [23].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This is known as the Weber\uc1\u8217\'92s Law. In other words, the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}human perception of difference in stimulus is often}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}measured as a fraction of the original stimulus, that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is, in a form \uc0\u916 I/I, which is called the Weber Fraction.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Chen et al. [43] proposed a local image descriptor,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}called Weber\uc1\u8217\'92s Law Descriptor for face recognition,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which was computed from Weber Fractions of pixels}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in a 3 \'d7 3 window. The proposed feature in Eq. (1)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}has also been used in other \uc0\u-1279 elds such as remote}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}sensing, where the Normalized Difference Vegetation}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Index (NDVI) [44] is de\uc0\u-1279 ned as the difference to sum}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ratio between the visible red and the near infrared}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}spectra to estimate the green vegetation coverage.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The NPD feature has a number of desirable properties. First, the NPD feature is antisymmetric, so either}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) or f (y, x) is adequate for feature representation, resulting in a reduced feature space. Therefore,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in an s \'d7 s image patch (vectorized as p \'d7 1, where}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}p = s \'b7 s), NPD feature f (xi , xj ) for pixel pairs}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1 \uc0\u8804  i < j \uc0\u8804  p is computed, resulting in d = p(p \uc0\u8722  1)/2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features. For example, in a 20\'d720 face template, there}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are (20 \'d7 20) \'d7 (20 \'d7 20 \uc0\u8722  1)/2 = 79, 800 NPD features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in total. We call the resulting feature space the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature space, denoted as \uc0\u937 npd (\uc0\u8712  Rd ).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Second, the sign of f (x, y) is an indicator of the ordinal relationship between x and y. Ordinal relationship has been shown to be an effective encoding for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}object detection and recognition [25], [26], [28] because}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ordinal relationship encodes the intrinsic structure of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}an object image and it is invariant under various}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}illumination changes [25]. However, simply using the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}sign to encode the ordinal relationship is likely to be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}sensitive to noise when x and y have similar values.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In the next section we will show how to learn robust}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ordinal relationships with NPD features.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Third, the NPD feature is scale invariant, which is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}expected to be robust against illumination changes.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This is important for image representation, since illumination change is always a troublesome issue for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}both object detection and recognition.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fourth, as shown in Appendix A, the NPD feature}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f(x,y) is bounded in [-1,1]. The bounded property}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}makes the NPD feature amenable to histogram binning or threshold learning in tree-based classi\uc0\u-1279 ers [1].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 2 shows that f (x, y) is a bounded function and it}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}de\uc0\u-1279 nes a nonlinear surface.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Theorem 1 (Reconstruction): Given the NPD feature vector f = (f (x1 , x2 ), f (x1 , x3 ), . . . ,f (xp\uc0\u8722 1 , xp ))T}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u8712  \uc0\u937 npd , the original image intensity values I =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(x1 , x2 , . . . , xp )T can be reconstructed up to a scale}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}factor.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 2. A plot of the NPD function f (x, y).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The proof of Theorem 1 is shown in Appendix B,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which also gives a linear-time approach to reconstruct}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the original image up to a scale factor. Theorem 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}states that each point in the feature space \uc0\u937 npd corresponds to a group of intensity-scaled images in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the original pixel intensity space. In contrast, the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}scale invariance property says that all intensity-scaled}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images are \uc1\u8220\'93compressed\uc1\u8221\'94 to a point in the bounded}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature space \uc0\u937 npd . Therefore, \uc0\u937 npd is a feature space}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is invariant to scale variations, but it carries all}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the necessary information from the original space.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FOR FACE DETECTION}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.1 Learning Object Structures}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Ordinal relationship [25] is a well-known simple and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}basic concept: it compares the brightness of any two}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}image regions, and encodes the result with 1 (brighter)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}or 0 (darker) accordingly. Sinha [25] showed that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ordinal features can represent the intrinsic structure of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}objects such as a human face, and they are insensitive}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to illumination changes. Instead of encoding ordinal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}relationship between two image regions, in this paper,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}we learn robust ordinal relationships between pairs of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pixels via the NPD feature. For a face pattern which is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}well structured, automatically learned combinations}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of ordinal features may represent a face better than}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}manual con\uc0\u-1279 gurations. Therefore, we propose to learn}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a combination of simple ordinal features by boosted}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}regression trees [33]. By providing a training set of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face and nonface images, a weak classi\uc0\u-1279 er is learned}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}by a regression tree. At each node, the tree checks}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the optimal ordinal feature value, and then passes the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}input data to the next branch accordingly. See Fig. 3.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Regression tree is also well suited for face detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with arbitrary pose variations, since similar views can}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}be clustered in the same leaf node of the tree.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Ordinal relationship can always be generated by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the default threshold of 0, but it will be sensitive to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}noise especially when the two pixels to be compared}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}have similar values. In this paper, we learn robust}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ordinal relationships and their combinations by learning regression trees with NPD features. In this way,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}regression trees not only learn optimal NPD features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 3. Learning and combining ordinal features in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a regression tree. Left: four pixelwise ordinal features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are automatically selected in the learning process.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Right: the four features are optimally combined in a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}regression tree for face/nonface prediction.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}at each branch node, but also learn optimal thresholds}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for splitting. Generally, one of the following two cases}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are leaned for each NPD feature at a branch node:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}< \uc0\u952 1 < 0,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(2)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u8805  \uc0\u952 2 > 0,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(3)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}where \uc0\u952 1 and \uc0\u952 2 are the thresholds. Eq. (2) applies if}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the object pixel x is notably darker than pixel y, while}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Eq. (3) covers the case when pixel x is notably brighter}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}than pixel y. The learned thresholds allow the ordinal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}encodings in the learned regression trees to represent}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the intrinsic object structure. To learn such regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}trees, we use the CART algorithm [33] with the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Face Detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Given that the proposed NPD features contain redundant information, we also apply the AdaBoost}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}algorithm to select the most discriminative features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and construct strong classi\uc0\u-1279 ers [1]. We adopt the Gentle AdaBoost algorithm [2] to learn the NPD feature}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}based regression trees.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}As in [1], a cascade classi\uc0\u-1279 er is further learned for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}rapid face detection. We only learn one single cascade}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er for unconstrained face detection robust to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}occlusions and pose variations. This implementation}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}has the advantage that there is no need to label the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pose of each face image manually or cluster the poses}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}before training the detector. In the learning process,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the algorithm automatically divides the whole face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}manifold into several sub-manifolds by regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}trees.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Below is a summary of how the proposed method}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}handles the unconstrained face detection problem.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 Pose. Pose variations are handled by learning}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD features in boosted regression trees, where}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}different views can be automatically partitioned}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}into different leaves of the regression trees.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 Occlusion. In contrast to Haar-like features that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are sensitive to occlusions because of large support [18], NPD features are computed by only}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}two pixel values, making them robust to occlusion.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Illumination. Since NPD features are scale invariant, they are robust to illumination changes.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Blur or low image resolution. Because the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features involve only two pixel values, they do}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not require rich texture information on the face.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This makes NPD features effective in handling}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}blurred or low resolution face images.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Speed Up}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}To further speed up the proposed NPD face detector,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}we develop the following two techniques. First, for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}8-bit gray images, we build a 256 \'d7 256 look up}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}table to store pre-computed NPD features. This way,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}computing f (x, y) in Eq. 1 only requires one memory}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}access from the look up table.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Second, the learned face detection template (e.g.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}20 \'d7 20 used in this paper) can be easily scaled to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}enable multiscale face detection. So, we pre-compute}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}multiscale detection templates and apply them to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detect faces at various scales. This way, iterative rescaling of images for multiscale detection is avoided.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}E XPERIMENTS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We evaluate the performance of the NPD face detector on three public-domain databases, FDDB [3],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}GENKI [45], and CMU-MIT [34]. We also provide}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}an analysis of the proposed method, report the face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection speed, and report unconstrained face detection performances under illumination variations, pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}variations, occlusion, and blur, respectively.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.1 Implementation of NPD Face Detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}A subset of the training data2 in [13] was used to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}train our detector, including 12,102 face images and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}12,315 nonface images (some private face images and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the Corel5k nonface images were not available, so}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}they could not be used). Fig. 4 shows some example}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face and nonface images from this training dataset.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The detection template is 20 \'d7 20 pixels. The detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade contains 15 stages, and for each stage, the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}target false accept rate was 0.5, with a detection rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of 0.999. For the depth of the regression tree, we}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}set a constraint that each leaf node must contain at}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}least (1/16)th of the total number of training samples.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Under this constraint, the tree depth is at most 5,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and in the test phase at most 4 NPD features need}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to be computed for each regression tree. The \uc0\u-1279 rst}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1279 ve stages of our detector include 3, 4, 6, 7, 9 weak}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 ers, respectively. Fig. 5 shows the NPD features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}learned in the three regression trees in the \uc0\u-1279 rst stage.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}It can be observed that most of the learned features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are around eyes, eyebrows, and nose. In addition, the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2. https://sites.google.com/site/leeplus/publications/}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}facedetectionusingsurfcascade}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 4. Example face (left) and nonface (right) images}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}from [13] for face detector training.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Tree 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Tree 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Tree 3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 5. The learned NPD features by boosting regression trees in the \uc0\u-1279 rst stage of the cascade.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features in the three regression trees are distributed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in different parts of the facial region. This is because,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in the boosting scheme, all samples are reweighted}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}when a weak classi\uc0\u-1279 er is learned, so that the next}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}weak classi\uc0\u-1279 er can focus on the training samples that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can not be correctly classi\uc0\u-1279 ed in the current step. The}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face shown in Fig. 5 is a frontal face, but it should be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}kept in mind that the face can have arbitrary pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}variations, and some learned features may be only}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}effective for a speci\uc0\u-1279 c pose.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In the test stage, a scale factor of 1.2 was set for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}multiscale detection. A postprocessing method similar}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to the OpenCV face detection module was implemented, which merges nearby detections by the disjoint}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}set algorithm. For each detected face, we summarized the scores of AdaBoost classi\uc0\u-1279 ers in all stages}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the cascade to be the \uc0\u-1279 nal score; this score was}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}used to generate the Receiver Operating Characteristic}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(ROC). We used three public face databases, FDDB [3],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}GENKI [45], and CMU-MIT [34], to evaluate our face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection algorithm.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Evaluation on FDDB Database}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The FDDB dataset [3] covers challenging scenarios for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection. Images in FDDB comes from the Faces}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in the Wild dataset [46], which is a large collection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of Internet images collected from the Yahoo News. It}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}contains 2,845 images with a total of 5,171 faces, with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a wide range of challenging scenarios including arbitrary pose, occlusions, different lightings, expressions,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}low resolutions, and out-of-focus faces. All faces in the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database have been annotated with elliptical regions.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 1 shows some examples of the annotated faces}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}from the FDDB database.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}For benchmark evaluation, Jain and LearnedMiller [3] provided an evaluation code for a compari-}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}son of different face detection algorithms. There are two metrics for performance evaluation based on ROC:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discrete score metric and continuous score metric,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which correspond to coarse match (similar to previous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}evaluations in the face detection literature) and precise match, respectively, between the detection and the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ground truth. Two experimental setups are proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in [3]. The \uc0\u-1279 rst experiment (EXP-1) requires a 10-fold}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cross-validation, while the second experiment (EXP-2)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}allows unrestricted training, which means that images}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}outside FDDB can be used for face detector training.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We followed both experimental protocols. For EXP1, we trained 10 face detectors, with the same settings}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}described in Section 5.1, and tested them separately}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}using 10-fold cross-validation. On average, we used}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}about 4,500 face images annotated in FDDB to train}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a single face detector. Fig. 6 shows some face images}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}that were cropped from the FDDB database for training our face detectors. Since FDDB does not provide a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}set of nonface images, we replaced all annotated face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}regions with black patches in the FDDB images and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}then used the resulting images to bootstrap nonface}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}samples. Fig. 7 illustrates such modi\uc0\u-1279 ed images.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}For EXP-2, we used the detector trained with data}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}outside FDDB, as described in the previous subsection. For evaluation, this detector was applied on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}each subset of the FDDB database separately, and an}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}average performance is reported.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We compared our method with state-of-the-art results reported on the FDDB website3 . The ROC curves}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of various algorithms are depicted in Fig. 8 for the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discrete score metric and in Fig. 9 for the continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}score metric. Note that all the baseline results are for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3. http://vis-www.cs.umass.edu/fddb/results.html}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}50%, Smooth NPD, EXP\uc0\u8722 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}47%, Smooth NPD, EXP\uc0\u8722 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}45%, NPD, EXP\uc0\u8722 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}38%, NPD, EXP\uc0\u8722 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}27%, Zhu\uc0\u8722 Ramanan [42]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}25%, Shenzhen University}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}25%, Illuxtech Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5%, VJGPR [40]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3%, Mikolajczyk et al. [49][3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3%, Olaworks Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3%, SURF Cascade [13]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1%, Viola\uc0\u8722 Jones [1][3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, XZJY [43]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, Koestinger et al. [50]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, Segui et al. [51]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a, PEP [44]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a, Subburaman\uc0\u8722 Marcel [39]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 7. Modi\uc0\u-1279 ed images from the FDDB database [3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for bootstrapping nonface samples.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 8. ROC curves for face detection on the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3] with the discrete score metric.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}From the discrete score metric results shown in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 8, it can be observed that the proposed method}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}34%, Smooth NPD, EXP\uc0\u8722 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}31%, Smooth NPD, EXP\uc0\u8722 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}30%, NPD, EXP\uc0\u8722 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}26%, NPD, EXP\uc0\u8722 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}21%, Zhu\uc0\u8722 Ramanan [42]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}17%, Shenzhen University}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}16%, Illuxtech Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5%, Olaworks Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3%, VJGPR [40]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2%, Mikolajczyk et al. [49][3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2%, SURF Cascade [13]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1%, Viola\uc0\u8722 Jones [1][3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, XZJY [43]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, Koestinger et al. [50]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0%, Segui et al. [51]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a, PEP [44]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a, Subburaman\uc0\u8722 Marcel [39]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 6.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Face images cropped from the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}EXP-2, because we did not \uc0\u-1279 nd any result following}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the EXP-1 protocol. In both Figs. 8 and 9, the curve}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}labels in the legend are sorted in descending order of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the detection rates at zero false positives (FP=0). Note}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}also that, on average, FP=285 generally means one}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}false detection per image for the FDDB experiments.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Therefore, the useful FPs are in the range [0,500]; we}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}show the X axis in logarithmic scale to emphasize}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the performance at low FPs. Among the baseline}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}methods, \uc1\u8220\'93Olaworks Inc.\uc1\u8221\'94 and \uc1\u8220\'93Illuxtech Inc.\uc1\u8221\'94 are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}two commercial detectors. Their methods, as well as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the method of \uc1\u8220\'93Shenzhen University\uc1\u8221\'94, have not been}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}published. \uc1\u8220\'93SURF Cascade\uc1\u8221\'94 is the SURF descriptor}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}based cascade method proposed by Li et al. in [13],}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is the best published result at low false positives to date. The method of Zhu-Ramanan [40] was}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}evaluated by the FDDB team, and the result, reported}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on the FDDB website, is now the state of the art}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}among published methods. For the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector, besides scaling the detection template}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in a nearest neighbor fashion, we also tried building}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the image pyramid representation by the default imresize function in MATLAB, and applied the 20 \'d7 20}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection template. Since this function uses the bicubic}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}interpolation method with antialiasing, we call the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}resulting detector \uc1\u8220\'93Smooth NPD\uc1\u8221\'94.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 9. ROC curves for face detection on the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3] with the continuous score metric.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 10. Detected faces in the FDDB database [3] by the proposed NPD method. Green boxes are detections by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the NPD detector, while red ellipses are ground truth annotations.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}outperforms all the baseline methods except Olaworks Inc. However, the proposed NPD detector is much}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}better than Olaworks\uc1\u8217\'92 detector when FP < 10. In}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}fact, when FP=0 (shown in the legend), the proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD detector detects 45% of the annotated FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces in coarse sense (50% overlap with ground truth),}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}while the detection rates of all baseline detectors are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}below 30%. Note that with a sub training set that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}was previously used for SURF Cascade [13], NPD for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}EXP-2 shows much better performance than SURF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Cascade. Further, the Smooth NPD is slightly better}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}than NPD, but with an additional cost of smoothing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}computation. It is also observed that the results of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the NPD detectors trained for EXP-1 and EXP-2 are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}comparable, though the training data size for EXP2 is several times larger than that for EXP-1. This}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}result indicates that FDDB contains representative}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images for unconstrained face detection. However, it}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is not easy to handle all this data in training a single}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detector (recall the large variations in face appearance}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in Fig. 6). Note that the generic NPD features are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}learned in regression trees to divide and conquer the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}complex face manifolds.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Similar observations can be found in Fig. 9 for the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}continuous score metric, except that Zhu-Ramanan is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}slightly better than the proposed method when FP> 5,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 9}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}TABLE 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Comparison of detection rates (%) with both discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and continuous metrics for EXP-2 on the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3]*}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Discrete Metric}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Smooth NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Zhu-Ramanan [40]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Shenzhen University}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Illuxtech Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}VJGPR [38]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Mikolajczyk et al. [47] [3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Olaworks Inc.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}SURF Cascade [13]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Viola-Jones [1] [3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}XZJY [41]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Koestinger et al. [48]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Segui et al. [49]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}PEP [42]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Subburaman-Marcel [37]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}*}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Continuous Metric}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}FP = 100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}47.23}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}45.32}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}27.38}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}24.87}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}24.56}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.58}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3.25}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2.94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2.59}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1.39}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.31}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.19}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.00}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}70.41}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}67.47}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}63.88}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}59.06}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}60.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}15.76}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10.23}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}67.84}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}48.27}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10.02}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}7.91}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}21.47}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}15.08}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}8.43}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.54}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}75.38}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}73.72}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}73.08}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}72.50}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}68.86}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}51.00}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}33.28}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}82.58}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}70.01}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}32.64}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}67.51}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}57.03}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}67.94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}73.35}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}17.25}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}31.26}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}29.99}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}21.25}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}16.51}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}16.50}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2.95}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2.10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.79}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1.60}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.90}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.19}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.14}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.00}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}n/a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}46.78}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}44.95}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}48.62}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}39.12}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}40.82}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10.20}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}6.61}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}45.18}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}30.21}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}6.48}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4.99}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}15.38}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}9.78}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.38}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.36}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}50.60}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}49.63}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}55.40}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}48.05}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}47.01}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}33.16}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}21.67}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}53.34}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}44.36}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}21.26}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}43.40}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}40.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}43.76}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}47.30}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}11.27}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Red numbers represents the best results, while blue numbers are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the second best results. Results for Mikolajczyk et al. [47] and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Viola-Jones [1] are reported in [3]. Results for Zhu-Ramanan [40]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are evaluated by the FDDB team and reported on their website.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and \uc1\u8220\'93Smooth NPD, EXP-1\uc1\u8221\'94 outperforms Olaworks}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Inc. Table 1 shows a comparison of detection rates for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}EXP-2 on the FDDB database at FP=0, 10, and 100. It}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is promising that at low false positives, the proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method is either much better than the baseline methods, or comparable to the best performers.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 10 shows some examples of detected faces in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the FDDB database by the proposed NPD method.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Rotated, occluded, and out-of-focus faces can be successfully detected by the proposed method as shown}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in Fig. 10. Some occluded faces (e.g. 4th row and 2nd}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}column in Fig. 10) and blurred faces (e.g. top-right}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}image in Fig. 10) that are not annotated in the ground}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}truth can still be detected by the proposed method.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}However, there are a number of faces that cannot be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detected by the proposed method, especially in very}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}crowded scenes (see the 1st image and the 3rd image}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in row 1, and the bottom-right image in Fig. 10).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.3 Evaluation on GENKI Database}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The GENKI database [45] was collected by the Machine Perception Laboratory, University of California,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}San Diego. We evaluated the current release of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}GENKI database, GENKI-R2009a, on its SZSL subset, which contains 3,500 images collected from the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Internet. These images include a wide range of backgrounds, illumination conditions, geographical locations, personal identity, and ethnicity. Some examples}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of face images from the GENKI database are shown in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 12, with labeled detections by the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method. Most images in the GENKI dataset contain}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}only one face. In that sense, the GENKI dataset is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not as challenging as the FDDB dataset. Some of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images in the GENKI-SZSL dataset contain faces that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 11. ROC curves for face detection on the GENKISZSL dataset [45] with (a) discrete and (b) continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}score metrics.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are not labeled, therefore they are not suitable for the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection evaluation task. After removing such}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}unlabeled images, we are left with 3,270 images for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection evaluation. For performance evaluation,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}it is not fair to apply the learned detector described}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in Section 5.1, because the training data used for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}that detector contained face images from the GENKI}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database4 . Therefore, we used the NPD face detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}trained on the \uc0\u-1279 rst fold of the FDDB 10-fold cross}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}validation to evaluate the GENKI database. We also}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}evaluated the Viola-Jones face detector implemented in OpenCV 2.4, and a commercial face detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}PittPatt [50]. We again used the benchmark evaluation code by in [3] for performance evaluation, but}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}slightly modi\uc0\u-1279 ed the code for allowing ground truth}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}annotations as rectangles. The ROC curves of the three}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}methods are shown in Fig. 11 for both the discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and continuous score metrics. The results show that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the proposed NPD face detector performs much better}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}than both the Viola-Jones and PittPatt face detectors.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Evaluation on CMU-MIT Database}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The CMU-MIT face dataset [34] is one of the early}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}benchmark for face detection. The CMU-MIT frontal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face data set contains 130 gray-scale images with a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}total of 511 faces, most of which are not occluded.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We applied the same NPD detector described in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Subsection 5.1 on this database. We also used the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}modi\uc0\u-1279 ed benchmark evaluation code from [3] with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the discrete score metric for performance evaluation.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 13 shows the ROC curves for the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector, the Soft cascade method [51], the SURF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade method [13], and the Viola-Jones detector [1].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The results show that, compared to the Viola-Jones}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}frontal face detector, the NPD detector performs better}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}when the number of false positives, FP < 50, while}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}it is slightly worse than Viola-Jones at higher FPs.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Compared to the SURF cascade detector, the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detector is better when FP < 3, but SURF cascade}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method outperforms NPD at higher FPs. Note that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}4. This training data is provided by the authors of [13]. We}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cannot remove the GENKI face images from this training data,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}because we have access to only the raw face images in binary}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}format; we do not know the corresponding \uc0\u-1279 lenames and sources.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 12. Detected faces in the GENKI-SZSL dataset [45] by the proposed NPD method.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 14. Detected faces in the CMU-MIT dataset [34] by the proposed NPD method.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 13. ROC curves for face detection on the CMUMIT dataset [34].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the SURF cascade method uses a face template of size}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}40 \'d7 40 pixels, which is four times larger than our}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection template (20 \'d7 20 pixels). Generally, a}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}larger face template contains more features for face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}description, but is computationally more expensive}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and may have a limitation in detecting blurred faces.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In addition, the proposed NPD method is not as good}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}as the Soft cascade, the state-of-the-art method on the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}CMU-MIT dataset. Still, the proposed NPD method}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can detect about 80% of the frontal faces without any}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}false positives, which is promising since we did not}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}train a frontal face detector. Some of the detected}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces in the CMU-MIT dataset by the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method are shown in Fig. 14.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Analysis of the Proposed Face Detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Since the proposed face detector is a combination of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}regression trees and the NPD features, it is instructive}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to determine the contribution of each of these two}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}components. In the following, we trained all compared face detectors on the same training set and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade training settings described in Section 5.1.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}First, we trained a face detector based on the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features, but with the stump classi\uc0\u-1279 er [1], a basic tree}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er with only one splitting node. As shown in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Table 2, the stump classi\uc0\u-1279 er based detector contains}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1,597 weak classi\uc0\u-1279 ers. In contrast, the regression tree}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}based detector contains 176 weak classi\uc0\u-1279 ers, indicating that combining NPD features in a regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tree is much more effective in constructing a weak}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er for AdaBoost learning. Furthermore, in cascade processing, each scanning subwindow needs}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to evaluate 36.5 NPD features, on average, for the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}stump classi\uc0\u-1279 er based detector. On the other hand,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for the regression tree based detector, only 34.4 NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features, on average, need to be evaluated, which}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}means that using regression tree does not increase the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}average computation cost. The face detectors based}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on the stump classi\uc0\u-1279 er and the regression tree were}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tested on the FDDB database. The ROC curves of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}these two detectors are shown in Fig. 15 for both}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the discrete score metric and continuous score metric.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}As illustrated, using regression trees instead of stump}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 er improves the face detection performance by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}about 2% \uc1\u8211\'96 10% for discrete metric and 1% \uc1\u8211\'96 7%}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 11}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.65}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD\uc0\u8722 Tree}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD\uc0\u8722 Stump}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.45}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.45}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.35}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.75}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD\uc0\u8722 Tree}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD\uc0\u8722 Stump}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 15. Comparison of NPD face detectors based on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}stumps and regression trees on the FDDB database [3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with (a) discrete and (b) continuous score metrics.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}TABLE 2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Comparison of detector complexity.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar LBP POF NPD-stump NPD-tree}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}#weak classi\uc0\u-1279 ers}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}150 108}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}276}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1,597}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}176}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}#features learned}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1,763 1,269 3,082}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1,597}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2,035}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}#feature evaluations 33.9 30.4 44.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}36.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}34.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for continuous metric. The improvement is larger at}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}smaller false positives.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Next, we \uc0\u-1279 xed the regression tree based weak}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}learner, but tried three other local features, namely}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar-like features [1], LBP [52], and pixelwise ordinal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature (POF) [30]. Since LBP is a discrete label, we}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}treated it as a categorical variable in the regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tree learning, that is, for branching at each tree node,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the algorithm \uc0\u-1279 nds the optimal criterion that splits}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the discrete LBP codes into two groups. Using the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}same training set as in Section 5.1, we trained the three}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detectors using Haar, LBP, and POF, respectively. The}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}model complexity of these detectors is summarized}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in Table 2. It can be observed that, the NPD model}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}appears to be more ef\uc0\u-1279 cient than the POF model,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}though it requires slightly more feature evaluations}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}than the Haar and LBP models. However, it should be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}noted that the computation of Haar-like features requires computing integral images, while for LBP, each}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature needs to compare 8 pairs of pixels and convert the resulting binary string to the corresponding}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}decimal number. In contrast, using look up tables as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}aforementioned, computing the NPD feature requires}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}only one memory access.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The four detectors with different local features were}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tested on the FDDB database, and the corresponding}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ROC curves are shown in Fig. 16 for both the discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and continuous score metrics. The NPD detector performs better than the Haar, LBP, and POF detectors}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with the same regression tree based weak learners.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The performance improvements due to NPD features}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}over Haar, LBP, and POF features are about 6%,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10%, and 6%, respectively, for discrete metric, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}about 4%, 6%, and 4%, respectively, for continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}metric, at FP=1. NPD is better than POF, because}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with NPD features the regression tree learns optimal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}thresholds to form more robust ordinal rules. NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.65}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}LBP}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}POF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.55}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.5}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}True Positive Rate}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.75}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.45}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Haar}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}LBP}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}POF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.35}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}10}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}100}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}False Positives}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}500}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 16. Comparison of different features in regression}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}tree based face detector on the FDDB database [3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with (a) discrete and (b) continuous score metrics.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}performs better than Haar and LBP, especially at}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}low false positives, indicating that combining optimal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pixel-level features in regression trees provides better}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discrimination between faces and nonfaces. On the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}other hand, one can also observe that except at low}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}false positives, NPD performs about the same or just}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}slightly better than Haar-like features and LBP.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We also tried a variation of NPD, de\uc0\u-1279 ned as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}. This is denoted as NPD2. With}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) = \uc0\u8730 x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x +y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the same setting as NPD, we trained another detector}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}based on NPD2. The testing results on FDDB are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}also shown in Fig. 16; the performances of NPD and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD2 are about the same, with NPD2 being slightly}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}better. However, considering that NPD is simpler than}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD2, we still suggest the formulation of Eq. (1).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.6 Evaluation Under Speci\uc0\u-1279 c Detection Challenge}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}In the following, we evaluate how the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector performs under illumination variation,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}pose variation, occlusion, and blur (or low resolution).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Note that these four challenges are often encountered}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}simultaneously in an image. In our selection of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}four subsets, one per speci\uc0\u-1279 c challenge, we focused on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the main source of variation in each image. For each}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}challenge, we selected 100 images from the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3] (examples are shown in Fig. 17), and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ran the NPD detector described in Subsection 5.1 on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}each subset separately. Fig. 18 shows that the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector performs the best on the illumination}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}subset. This is not surprising since the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features are robust against illumination variations.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Further, the NPD method performs better for face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images with pose variation than with occlusion or}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}blur. These results indicate that occlusion and blur}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are the two major challenges for unconstrained face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection, which have not been well addressed in the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}literature.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The NPD face detector is also compared with the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Viola-Jones face detector implemented in OpenCV 2.4,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and the commercial face detector PittPatt on the four}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}subsets of FDDB discussed above. The resulting ROC}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}curves with the discrete score metric are shown in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 19. These plots show that the proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 12}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) illumination}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(c) occlusion}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(d) blur}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 17. Example images and annotated faces for four}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}subsets extracted from the FDDB database [3].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) discrete}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) continuous}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 18. ROC curves of the proposed NPD face detector on the four subsets extracted from the FDDB}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}database [3] with (a) discrete and (b) continuous score}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}metrics.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector outperforms both the Viola-Jones and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the PittPatt face detectors on all the four subsets. The reasons for the superior performance of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}proposed method under illumination variations, pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}variations, occlusions, and blur, were discussed in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Subsection 4.2.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Detection Speed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}For handheld devices like mobile phones, the available resources for computation and memory are rather}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}limited. Therefore, face detector\uc1\u8217\'92s complexity and detection speed are very important for embedded systems. In this subsection, we report the detection speed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the proposed NPD face detector, compared with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the Viola-Jones5 face detector in OpenCV 2.4, which is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}known to be optimized for speed. The proposed NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detector is implemented in C++; the size of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}model trained in Section 5.1 is 41KB. Two platforms}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5. We have tested four models of the Viola-Jones face detector provided in OpenCV 2.4, and found that the \uc1\u8220\'93haarcascade frontalface alt\uc1\u8221\'94 model is the fastest, which was selected here}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for comparison.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(a) occlusion}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(c) illumination}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(b) pose}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(d) blur}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fig. 19. ROC curves for face detection on four subsets}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}from the FDDB database [3] with the discrete score}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}metric.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}were selected for this evaluation: (i) a normal desktop}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}PC with the Intel Core i5-2400 @3.1GHz CPU (4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cores, 4 threads), and (ii) a netbook with Intel Atom}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}N450 @1.6GHz processor (1 core, 2 threads), to simulate low-end devices. For face detection evaluation,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a video clip of the movie \uc1\u8220\'93Jobs\uc1\u8221\'94 was used. This video}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}clip shows a busy campus, with each frame containing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}from one to tens of faces. The length of the video}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}clip is about 2 minutes, containing 3,950 frames in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}total. The original resolution is 1280 \'d7 720. To test}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the detection speed at various resolutions, the original}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}video clip was cropped and resized to 1920 \'d7 1080,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}800\'d7600, and 640\'d7480. In this evaluation, the minimal}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face size to detect was set to 40 \'d7 40 pixels, and the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}scaling factor was 1.2. The multi threading technique}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}was enabled in both NPD and OpenCV detectors for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}parallel computation.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The test results (measured in terms of Frame Per}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Second, FPS) are shown in Table 3. Note that we only}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}calculated the face detection time, regardless of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}video decoding time. The detection speed of the SURF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade [13], a fast face detection algorithm, is also}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}compared in Table 3. The detection speed of the SURF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade algorithm is taken directly from [13], since}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}we do not have access to the code. The detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}parameters in [13] are the same as our algorithm,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}except that authors in [13] used the Intel Core-i7 CPU}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for the desktop computer. From Table 3 it can be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}observed that the NPD detector is much faster than}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}both the OpenCV and SURF cascade detectors. On}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Atom N450 processor, the detection speed of the NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detector is about 9 times faster than the detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}speed of the OpenCV detector; on i5 processor the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}speed of the NPD detector is about 7 times the speed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the OpenCV detector.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Table 3 shows that the NPD detector can run in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 13}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}TABLE 3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Comparison of face detection speed (as FPS).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}CPU}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Resolution NPD OpenCV SURF [13]*}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}640 \'d7 480}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}19.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2.1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}5.8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Atom N450}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}800 \'d7 600}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}12.1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}@1.6GHz}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1280 \'d7 720}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}6.8}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(1 core, 2 threads) 1920 \'d7 1080 3.0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}640 \'d7 480 177.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}24.4}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}71.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}i5-2400}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}800 \'d7 600 112.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}16.2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}@3.1GHz}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1280 \'d7 720 63.3}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}8.9}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(4 cores, 4 threads) 1920 \'d7 1080 29.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}3.6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}*}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8220\'93-\uc1\u8221\'94 means data is not available for the SURF detector [13].}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This is because we do not have access to the code, and [13]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}only reports detection speed at resolution 640\'d7480 or lower.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}R EFERENCES}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[1]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[2]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[3]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[4]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[5]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[6]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}real-time (29.6 FPS) on i5 desktop PC for processing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1920 \'d7 1080 high de\uc0\u-1279 nition videos. For the standard}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}VGA (640 \'d7 480) videos, the NPD detector on i5 processor can detect faces at even faster speed (177.6 FPS).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}On the low-end Atom platform, the NPD detector can}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}still run in near real-time (19.4 FPS) for processing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}VGA videos. The reasons for the high processing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}speed of NPD are two folds. First, the NPD feature}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is simple, involving only two pixels. Further with the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}look up table technique, the evaluation of each NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}feature requires only one memory access. Second, the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD feature can be easily scaled to various sizes}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of detection templates. Therefore, pre-calculating and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}storing multiscale templates can speed up detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}because rescaling the input image is avoided.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[7]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[8]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[9]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[10]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[11]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[12]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[13]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[14]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}6}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}S UMMARY}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}AND}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}F UTURE W ORK}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We have proposed a fast and accurate method for face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection in cluttered scenes. The method is based}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on the normalized pixel difference (NPD) feature in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}conjunction with boosted regression trees. An analysis}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of NPD feature shows that it possesses properties}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of scale invariance, boundedness, and reconstruction}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}ability. We have developed a method for learning the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}optimal set of NPD features and their combinations.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}As a result, a single cascade AdaBoost classi\uc0\u-1279 er is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}able to achieve promising results for face detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}with large pose variations and occlusions. Evaluations on three public domain databases, namely FDDB, GENKI, and CMU-MIT show that the proposed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method outperforms state-of-the-art methods for unconstrained face detection. The proposed NPD face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detector can process 1920 \'d7 1080 video frames in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}realtime, which is about 6 times faster than the ViolaJones face detector implemented in OpenCV 2.4. The}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}reported results also show that occlusions and blur are}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}two big challenges for face detection. Our future work}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}will use the NPD feature and the classi\uc0\u-1279 er learning}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}method for other applications such as face attribute}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}classi\uc0\u-1279 cation (e.g. pose estimation, age estimation, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}gender classi\uc0\u-1279 cation) and pedestrian detection.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[15]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[16]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[17]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[18]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[19]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[20]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[21]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[22]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[23]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[24]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[25]}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}P. Viola and M. Jones, \uc1\u8220\'93Rapid object detection using a boosted}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade of simple features,\uc1\u8221\'94 in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2001.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}J. Friedman, T. Hastie, and R. Tibshirani, \uc1\u8220\'93Additive logistic regression: a statistical view of boosting,\uc1\u8221\'94 The Annals of Statistics,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}vol. 28, no. 2, pp. 337\uc1\u8211\'96374, April 2000.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}V. Jain and E. Learned-Miller, \uc1\u8220\'93FDDB: A benchmark for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection in unconstrained settings,\uc1\u8221\'94 University of Massachusetts, Amherst, Tech. Rep. UM-CS-2010-009, 2010.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}R. Lienhart and J. Maydt, \uc1\u8220\'93An extended set of Haar-like}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}features for rapid object detection,\uc1\u8221\'94 in Proceedings of the IEEE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}International Conference on Image Processing, 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}S. Li, L. Zhu, Z. Zhang, A. Blake, H. Zhang, and H. Shum, \uc1\u8220\'93Statistical learning of multi-view face detection,\uc1\u8221\'94 in Proceedings}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the 7th European Conference on Computer Vision, 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}M. Jones and P. Viola, \uc1\u8220\'93Fast multi-view face detection,\uc1\u8221\'94 Mitsubishi Electric Research Lab TR-2003-96, 2003.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}B. Froba and A. Ernst, \uc1\u8220\'93Face detection with the modi\uc0\u-1279 ed}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}census transform,\uc1\u8221\'94 in Proceedings of the 6th IEEE International}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Automatic Face and Gesture Recognition, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}H. Jin, Q. Liu, H. Lu, and X. Tong, \uc1\u8220\'93Face detection using}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}improved LBP under bayesian framework,\uc1\u8221\'94 in Proceedings of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the 3rd International Conference on Image and Graphics, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}T. Mita, T. Kaneko, and O. Hori, \uc1\u8220\'93Joint Haar-like features for}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection,\uc1\u8221\'94 in Proceedings of the 10th IEEE International}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Computer Vision, vol. 2, 2005, pp. 1619\uc1\u8211\'961626.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}H. Zhang, W. Gao, X. Chen, and D. Zhao, \uc1\u8220\'93Object detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}using spatial histogram features,\uc1\u8221\'94 Image and Vision Computing,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}vol. 24, no. 4, pp. 327\uc1\u8211\'96341, 2006.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}L. Zhang, R. Chu, S. Xiang, S. Liao, and S. Z. Li, \uc1\u8220\'93Face detection based on multi-block LBP representation,\uc1\u8221\'94 in Proceedings}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the IAPR/IEEE International Conference on Biometrics, 2007.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}S. Yan, S. Shan, X. Chen, and W. Gao, \uc1\u8220\'93Locally assembled}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}binary (LAB) feature with feature-centric cascade for fast and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}accurate face detection,\uc1\u8221\'94 in Proceedings of IEEE Computer Society}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Computer Vision and Pattern Recognition, 2008.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}J. Li, T. Wang, and Y. Zhang, \uc1\u8220\'93Face detection using SURF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade,\uc1\u8221\'94 in ICCV BeFIT workshop, 2011.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}B. Wu, H. Ai, C. Huang, and S. Lao, \uc1\u8220\'93Fast rotation invariant}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}multi-view face detection based on real AdaBoost,\uc1\u8221\'94 in IEEE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Automatic Face and Gesture Recognition, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}S. Li and Z. Zhang, \uc1\u8220\'93Floatboost learning and statistical face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection,\uc1\u8221\'94 IEEE Transactions on Pattern Analysis and Machine}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Intelligence, vol. 26, no. 9, pp. 1112\uc1\u8211\'961123, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}C. Huang, H. Ai, Y. Li, and S. Lao, \uc1\u8220\'93High-performance rotation invariant multiview face detection,\uc1\u8221\'94 IEEE Transactions}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on Pattern Analysis and Machine Intelligence, vol. 29, no. 4, pp.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}671\uc1\u8211\'96686, 2007.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}K. Hotta, \uc1\u8220\'93A robust face detector under partial occlusion,\uc1\u8221\'94 in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}International Conference on Image Processing, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Y. Lin, T. Liu, and C. Fuh, \uc1\u8220\'93Fast object detection with occlusions,\uc1\u8221\'94 in Proceedings of the European Conference on Computer}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Vision, 2004, pp. 402\uc1\u8211\'96413.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Y. Lin and T. Liu, \uc1\u8220\'93Robust face detection with multi-class}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}boosting,\uc1\u8221\'94 in Proceedings of IEEE Computer Society Conference}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on Computer Vision and Pattern Recognition, 2005.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}J. Chen, S. Shan, S. Yang, X. Chen, and W. Gao, \uc1\u8220\'93Modi\uc0\u-1279 cation}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of the adaboost-based detector for partially occluded faces,\uc1\u8221\'94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in 18th International Conference on Pattern Recognition, 2006.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}L. Goldmann, U. Monich, and T. Sikora, \uc1\u8220\'93Components and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}their topology for robust face detection in the presence of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}partial occlusions,\uc1\u8221\'94 IEEE Transactions on Information Forensics}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and Security, vol. 2, no. 3, pp. 559\uc1\u8211\'96569, 2007.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}M. Yang, D. Kriegman, and N. Ahuja, \uc1\u8220\'93Detecting faces in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}images: A survey,\uc1\u8221\'94 IEEE Transactions on Pattern Analysis and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Machine Intelligence, vol. 24, no. 1, pp. 34\uc1\u8211\'9658, 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}E. H. Weber, \uc1\u8220\'93Tastsinn und gemeingefu\uc0\u776 hl,\uc1\u8221\'94 in Handwo\uc0\u776 rterbuch}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}der Physiologie, R. Wagner, Ed. Brunswick: Vieweg, 1846, pp.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}481\uc1\u8211\'96588.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}C. Zhang and Z. Zhang, \uc1\u8220\'93A survey of recent advances in face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection,\uc1\u8221\'94 Microsoft Research, Tech. Rep. MSR-TR-2010-66,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}June 2010.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}P. Sinha, \uc1\u8220\'93Qualitative representations for recognition,\uc1\u8221\'94 in Biologically Motivated Computer Vision Workshop, 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 14}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[26] J. Sadr, S. Mukherjee, K. Thoresz, , and P. Sinha, \uc1\u8220\'93Toward the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1279 delity of local ordinal encoding,\uc1\u8221\'94 in Proceedings of the Annual}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Neural Information Processing Systems, 2001.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[27] S. Baluja, M. Sahami, and H. Rowley, \uc1\u8220\'93Ef\uc0\u-1279 cient face orientation}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}discrimination,\uc1\u8221\'94 in International Conference on Image Processing,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}vol. 1, 2004, pp. 589\uc1\u8211\'96592.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[28] S. Liao, Z. Lei, X. Zhu, Z. Sun, S. Z. Li, and T. Tan, \uc1\u8220\'93Face}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}recognition using ordinal features,\uc1\u8221\'94 in Proceedings of the 1st}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}IAPR International Conference on Biometrics, Hong Kong, 2006.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[29] Y. Abramson, B. Steux, and H. Ghorayeb, \uc1\u8220\'93Yet even faster}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(YEF) real-time object detection,\uc1\u8221\'94 International Journal of Intelligent Systems Technologies and Applications, vol. 2, no. 2, pp.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}102\uc1\u8211\'96112, 2007.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[30] L. Wang, L. Ding, X. Ding, and C. Fang, \uc1\u8220\'932D face \uc0\u-1279 ttingassisted 3D face reconstruction for pose-robust face recognition,\uc1\u8221\'94 Soft Computing-A Fusion of Foundations, Methodologies and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Applications, vol. 15, no. 3, pp. 417\uc1\u8211\'96428, 2011.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[31] R. Lienhart, A. Kuranov, and V. Pisarevsky, \uc1\u8220\'93Empirical analysis}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of detection cascades of boosted classi\uc0\u-1279 ers for rapid object}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}detection,\uc1\u8221\'94 MRL, Intel Labs, Tech. Rep., May 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[32] S. Brubaker, J. Wu, J. Sun, M. Mullin, and J. Rehg, \uc1\u8220\'93On the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}design of cascades of boosted ensembles for face detection,\uc1\u8221\'94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Georgia Institute of Technology, Tech. Rep. GIT-GVU-05-28,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2005.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[33] L. Breiman, J. Friedman, R. Olshen, and C. J. Stone, Classi\uc0\u-1279 cation and Regression Trees. Chapman & Hall/CRC, 1984.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[34] H. Rowley, S. Baluja, and T. Kanade, \uc1\u8220\'93Rotation invariant}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}neural network-based face detection,\uc1\u8221\'94 in IEEE International}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Computer Vision and Pattern Recognition, 1998.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[35] E. Seemann, B. Leibe, and B. Schiele, \uc1\u8220\'93Multi-aspect detection}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}of articulated objects,\uc1\u8221\'94 in Proceedings of IEEE Computer Society}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Computer Vision and Pattern Recognition, 2006.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[36] T. Kim and R. Cipolla, \uc1\u8220\'93MCBoost: Multiple classi\uc0\u-1279 er boosting}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}for perceptual co-clustering of images and visual features,\uc1\u8221\'94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Proceedings of Neural Information Processing Systems, 2008.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[37] V. B. Subburaman and S. Marcel, \uc1\u8220\'93Fast bounding box estimation based face detection,\uc1\u8221\'94 in ECCV Workshop on Face Detection:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Where we are and what next, 2010.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[38] V. Jain and E. Learned-Miller, \uc1\u8220\'93Online domain adaptation of}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}a pre-trained cascade of classi\uc0\u-1279 ers,\uc1\u8221\'94 in IEEE Computer Society}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Conference on Computer Vision and Pattern Recognition, 2011.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[39] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, \uc1\u8220\'93Speeded-up}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}robust features (SURF),\uc1\u8221\'94 Computer Vision and Image Understanding, vol. 110, no. 3, pp. 346\uc1\u8211\'96359, 2008.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[40] X. Zhu and D. Ramanan, \uc1\u8220\'93Face detection, pose estimation,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and landmark localization in the wild,\uc1\u8221\'94 in IEEE Conference on}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Computer Vision and Pattern Recognition (CVPR), 2012.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[41] X. Shen, Z. Lin, J. Brandt, and Y. Wu, \uc1\u8220\'93Detecting and aligning}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}faces by image retrieval,\uc1\u8221\'94 in IEEE Conference on Computer Vision}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and Pattern Recognition (CVPR), 2013.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[42] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang, \uc1\u8220\'93Probabilistic}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}elastic part model for unsupervised face detector adaptation,\uc1\u8221\'94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}in IEEE International Conference on Computer Vision, 2013.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[43] J. Chen, S. Shan, C. He, G. Zhao, M. Pietika\uc0\u776 inen, X. Chen,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and W. Gao, \uc1\u8220\'93WLD: A robust local image descriptor,\uc1\u8221\'94 IEEE}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Transactions on Pattern Analysis and Machine Intelligence, vol. 32,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}no. 9, pp. 1705\uc1\u8211\'961720, Sept. 2010.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[44] F. Kriegler, W. Malila, R. Nalepka, and W. Richardson, \uc1\u8220\'93Preprocessing transformations and their effects on multispectral}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}recognition,\uc1\u8221\'94 in Proceedings of the Sixth International Symposium}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}on Remote Sensing of Environment, 1969, pp. 97\uc1\u8211\'96131.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[45] http://mplab.ucsd.edu, \uc1\u8220\'93The MPLab GENKI Database,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}GENKI-SZSL Subset.\uc1\u8221\'94}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[46] T. L. Berg, A. C. Berg, J. Edwards, and D. Forsyth, \uc1\u8220\'93Whos in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the picture,\uc1\u8221\'94 Advances in neural information processing systems,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}vol. 17, pp. 137\uc1\u8211\'96144, 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[47] K. Mikolajczyk, C. Schmid, and A. Zisserman, \uc1\u8220\'93Human detection based on a probabilistic assembly of robust part detectors,\uc1\u8221\'94 in European Conference on Computer Vision (ECCV), 2004.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[48] M. Ko\uc0\u776 stinger, P. Wohlhart, P. M. Roth, and H. Bischof, \uc1\u8220\'93Robust}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}face detection by simple means,\uc1\u8221\'94 in DAGM Computer Vision in}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Applications Workshop, 2012.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[49] S. Segu\uc0\u305 \uc0\u769 , M. Drozdzal, P. Radeva, and J. Vitria\uc0\u768 , \uc1\u8220\'93An integrated}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}approach to contextual face detection.\uc1\u8221\'94 in International Conference on Pattern Recognition Applications and Methods, 2012.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[50] PittPatt Software Developer Kit, Pittsburgh Pattern Recognition, Inc., http://www.pittpatt.com.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[51] L. Bourdev and J. Brandt, \uc1\u8220\'93Robust object detection via soft}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}cascade,\uc1\u8221\'94 in IEEE Computer Society Conference on Computer}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Vision and Pattern Recognition, vol. 2, 2005, pp. 236\uc1\u8211\'96243.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}[52] T. Ojala, M. Pietika\uc0\u776 inen, and T. Ma\uc0\u776 enpa\uc0\u776 a\uc0\u776 , \uc1\u8220\'93Multiresolution}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}gray-scale and rotation invariant texture classi\uc0\u-1279 cation with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}local binary patterns,\uc1\u8221\'94 IEEE Transactions on Pattern Analysis}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}and Machine Intelligence, vol. 24, no. 7, pp. 971\uc1\u8211\'96987, 2002.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}A PPENDIX A}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}B OUNDEDNESS}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}OF}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}NPD}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Lemma 1 (Boundedness): \uc0\u8704 x, y \uc0\u8805  0, the NPD feature f(x,y) is well bounded in [-1,1]. In addition,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) = 1 if and only if x > 0 and y = 0; and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) = \uc0\u8722 1 if and only if x = 0 and y > 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Proof: From the definition of NPD we know that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x \uc0\u8805  0, y \uc0\u8805  0, and f (0, 0) = 0 \uc0\u8712  [\uc0\u8722 1, 1]. When either x}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}or y is nonzero, for example, y \uc0\u8805  0 but x > 0, Eq. (1)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}can be reformulated as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2x}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u8722  1 \uc0\u8804  1. (a)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}=}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u8722 1=}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}1 + xy}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The inequality in Eq. (a) holds because y \uc0\u8805  0, and}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the last equality holds if and only if x > 0 and y =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0. Similarly, when x \uc0\u8805  0 but y > 0, Eq. (1) can be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}reformulated as}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x\uc0\u8722 y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}2}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f (x, y) =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}=1\uc0\u8722 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}=1\uc0\u8722  x}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u8805  \uc0\u8722 1. (b)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x+y}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}y +1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}The inequality in Eq. (b) holds because x \uc0\u8805  0, and the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}last equality holds if and only if x = 0 and y > 0. ?}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}A PPENDIX B}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}P ROOF OF T HEOREM 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Denote fij = f (xi , xj ). From Eq. (1) we have}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}fij (xi + xj ) = xi \uc0\u8722  xj .}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(c)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(fij \uc0\u8722  1)xi + (fij + 1)xj = 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(d)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Equivalently,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Therefore, we have the following set of linear equations}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Fx = 0,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(e)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}where}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1813 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f12 \uc0\u8722  1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812  f13 \uc0\u8722  1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812  \'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}F=\uc0\u-1812 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812  f1p \uc0\u8722  1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1812 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1811  \'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f12 + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f23 \uc0\u8722  1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f13 + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f23 + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1p + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}0}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7\'b7\'b7}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\'b7 \'b7 \'b7 fp\uc0\u8722 1,p + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1802 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1801 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc0\u-1800 }{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(f)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}is a sparse d \'d7 p matrix with each row containing}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}at most two nonzero entries. Furthermore, from the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}formulation of F we know that each row of F contains}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}at least one nonzero entry, because (fij \uc0\u8722  1) 6= (fij + 1)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}always holds for all i and j. Without loss of generality, let\uc1\u8217\'92s assume f12 + 1 6= 0. Then it follows that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1j +1 6= 0, \uc0\u8704 j. Because if \uc0\u8707 j such that f1j +1 = 0, then}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}from Lemma 1 we know that x1 = 0. This will further}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}lead to f12 +1 = 0, which violates the assumption that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f12 +1 6= 0. Therefore, the first p\uc0\u8722 1 rows in the matrix}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}F are linearly independent of each other.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}We will further prove that rank(F) = p \uc0\u8722  1. In fact,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}any row of the matrix F can be linearly expressed by}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}View publication stats}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the first p \uc0\u8722  1 rows. To show this, let\uc1\u8217\'92s denote the row}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}containing fij \uc0\u8722 1 and fij +1 by rij . We will show that}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}rij =}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}fij + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}fij \uc0\u8722  1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}r1i +}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}r1j ,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1i + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1j + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(g)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}holds for all i > 1 and j > i. In fact, it is easy to verify}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}that the above equation holds for all columns of rij ,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}r1i , and r1j after the first column. So, we only need}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to show that, for the first column, we have}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(f1i \uc0\u8722  1)(fij \uc0\u8722  1) (f1j \uc0\u8722  1)(fij + 1)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}+}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}= 0,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1i + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1j + 1}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(h)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is equivalent to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}f1i f1j fij \uc0\u8722  f1i + f1j \uc0\u8722  fij = 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}(i)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}This can be verified by substituting each feature with}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}its definition in Eq. (1).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Given that rank(F) = p \uc0\u8722  1, we know that the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}nullspace of F contains only one nonzero vector,}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}which is a solution to Eq. (e). Furthermore, from}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Lemma 1 we can infer that (fij \uc0\u8722 1)(fij +1) \uc0\u8804  0, hence}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Eq. (d) tells that xi xj \uc0\u8805  0, \uc0\u8704 i, j. Consequently, Eq. (e)}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}always has a nonnegative solution x\uc0\u770 , and all solutions}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}to Eq. (e) must be cx\uc0\u770 , where c is a scale factor.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}?}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}Given this proof, we make four observations below:}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 For a solution, c can be any real value, but to}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}satisfy the constraint that all pixel intensity values}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}are nonnegative, c should be positive.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 The solution to Eq. (e) spans a one-dimensional}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}subspace (the nullspace).}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 A specific solution can be obtained by assigning}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}x1 = 1 and solving for the other variables from}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}the first p \uc0\u8722  1 rows of Eq. (e) in linear time.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\uc1\u8226\'95 When the original image is x = 0, it can also be}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}reconstructed by cx\uc0\u770  where x\uc0\u770 i = 1, \uc0\u8704 i, and c = 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}However, in this case a solution with c > 0 is}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}not generally regarded as a scaled version of the}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}original image x = 0.}{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\s28\itap0{\s28\f0\fs24\lang1033{\*\listtag0}\page }{\s28\f0\fs24\lang1033{\*\listtag0}\par}}